models:
  - model: Azazelle/Yuna-7b-Merge
  - model: nlpguy/Hermes-low-tune-3.1
merge_method: slerp
base_model: Azazelle/Yuna-7b-Merge
parameters:
  t:
    - filter: self_attn
      value: [0, 0.25, 0.5, 0.75, 1]
    - filter: mlp
      value: [1, 0.75, 0.5, 0.25, 0]
    - value: 0.5 # fallback for rest of tensors
dtype: bfloat16
